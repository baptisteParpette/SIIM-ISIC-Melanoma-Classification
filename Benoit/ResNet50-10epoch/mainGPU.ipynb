{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Créer un écrivain SummaryWriter\n",
    "writer = SummaryWriter('runs/melanoma_experiment_1')  # Ajustez le chemin\n",
    "\n",
    "# Vérifier si CUDA est disponible, sinon utiliser le CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Architecture simplifiée du CNN\n",
    "class MelanomaCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelanomaCNN, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(16 * 112 * 112, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16 * 112 * 112)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Ensemble de données personnalisé\n",
    "class MelanomaDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, limit=None):\n",
    "        self.labels_df = pd.read_csv(csv_file).head(limit)  # Limite à 100 images\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.labels_df.iloc[idx, 0] + \".jpg\")\n",
    "        image = Image.open(img_name)\n",
    "        label = torch.tensor(self.labels_df.iloc[idx, 1], dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benoi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\benoi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Transformations pour les images\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Charger l'ensemble de données\n",
    "full_dataset = MelanomaDataset(csv_file='C:\\\\Users\\\\benoi\\\\Downloads\\\\isic-2020-resized\\\\train-labels.csv',  # Ajustez le chemin\n",
    "                                img_dir='C:\\\\Users\\\\benoi\\\\Downloads\\\\isic-2020-resized\\\\train-resized\\\\train-resized',     # Ajustez le chemin\n",
    "                                transform=transform)\n",
    "\n",
    "# Séparer l'ensemble de données en ensembles d'entraînement et de validation\n",
    "train_size = int(0.8 * len(full_dataset))  # 80% pour l'entraînement\n",
    "val_size = len(full_dataset) - train_size  # 20% pour la validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, pin_memory=True)\n",
    "\n",
    "# Charger ResNet50 pré-entraîné\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = model.to(device)  \n",
    "\n",
    "# Décongeler les dernières couches\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"layer4\" in name or \"fc\" in name:\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# Remplacer la dernière couche fc pour notre classification binaire\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model.fc = model.fc.to(device) \n",
    "\n",
    "# Fonction de perte et optimiseur\n",
    "weights = [0.1, 0.9]  # Poids pour chaque classe\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Scheduler pour le taux d'apprentissage\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Entraîner le modèle\n",
    "epochs = 15  # Augmenter le nombre d'époques\n",
    "for epoch in range(epochs):\n",
    "    # Entraînement\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data).squeeze()\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "    writer.add_scalar('Training Loss', total_loss / len(train_loader), epoch)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data).squeeze()\n",
    "            loss = criterion(outputs, target)\n",
    "            total_val_loss += loss.item()\n",
    "    print(f\"Validation Loss: {total_val_loss / len(val_loader)}\")\n",
    "    writer.add_scalar('Validation Loss', total_val_loss / len(val_loader), epoch)\n",
    "\n",
    "    # Mise à jour du taux d'apprentissage\n",
    "    scheduler.step()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), 'melanoma_model.pth')  # Ajustez le chemin\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
