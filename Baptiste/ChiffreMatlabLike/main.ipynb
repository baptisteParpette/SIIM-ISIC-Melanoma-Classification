{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from struct import unpack\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Reconstruction du modèle en PyTorch basé sur l'architecture du code MATLAB\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolution layer\n",
    "        self.conv = nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # Batch Normalization layer\n",
    "        self.batchnorm = nn.BatchNorm2d(20)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully Connected layer\n",
    "        self.fc = nn.Linear(20 * 12 * 12, 10)  # après le pooling, la taille de l'image est 12x12\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 20 * 12 * 12)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset for MNIST to read from IDX format and normalize on the fly.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_file, label_file):\n",
    "        self.image_file = image_file\n",
    "        self.label_file = label_file\n",
    "        \n",
    "        with open(label_file, 'rb') as f:\n",
    "            _, self.num_items = unpack('>II', f.read(8))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_items\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        with open(self.image_file, 'rb') as f:\n",
    "            f.seek(16 + index * 28 * 28)\n",
    "            image_data = np.frombuffer(f.read(28 * 28), dtype=np.uint8).reshape(1, 28, 28)\n",
    "        \n",
    "        with open(self.label_file, 'rb') as f:\n",
    "            f.seek(8 + index)\n",
    "            label_data = np.frombuffer(f.read(1), dtype=np.uint8)[0]\n",
    "        \n",
    "        image_data = (torch.tensor(image_data, dtype=torch.float32) / 255.0) * 2 - 1\n",
    "        return image_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n",
      "CNN(\n",
      "  (conv): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (batchnorm): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=2880, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create custom dataset instances\n",
    "train_custom_dataset = MNISTDataset(\"data/train-images-idx3-ubyte\", \"data/train-labels-idx1-ubyte\")\n",
    "test_custom_dataset = MNISTDataset(\"data/t10k-images-idx3-ubyte\", \"data/t10k-labels-idx1-ubyte\")\n",
    "\n",
    "# Create DataLoader instances   \n",
    "batch_size=64\n",
    "\n",
    "train_loader = DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check the size of the datasets\n",
    "print(len(train_loader.dataset), len(test_loader.dataset))\n",
    "\n",
    "\n",
    "# Créer l'instance du modèle\n",
    "cnn_model = CNN()\n",
    "if torch.cuda.is_available():\n",
    "    cnn_model = cnn_model.cuda()\n",
    "\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la fonction de perte et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=lr)\n",
    "\n",
    "# Pour enregistrer la perte et la précision\n",
    "train_losses = []\n",
    "accuracies = []\n",
    "\n",
    "# Créer un écrivain TensorBoard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Ajouter le modèle à TensorBoard\n",
    "data_iter = iter(train_loader)\n",
    "images, _ = next(data_iter)\n",
    "if torch.cuda.is_available():\n",
    "    images = images.cuda()\n",
    "writer.add_graph(cnn_model, images)\n",
    "\n",
    "# Entraîner le modèle\n",
    "for epoch in range(epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Enregistrer la perte dans TensorBoard\n",
    "    writer.add_scalar('Loss/train', total_loss / len(train_loader), epoch)\n",
    "\n",
    "    # Évaluation de la précision et visualisation des prédictions\n",
    "    cnn_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            outputs = cnn_model(data)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            # Ajouter quelques images et prédictions à TensorBoard\n",
    "            if batch_idx == 0:  # Juste pour le premier lot\n",
    "                for i in range(10):  # Ajoutons 10 images et leurs prédictions\n",
    "                    img = data[i] / 2 + 0.5  # un-normalize\n",
    "                    writer.add_image(f\"Image {i}\", img, epoch)\n",
    "                    writer.add_text(f\"Prediction {i}\", f\"Predicted: {predicted[i]}, True: {target[i]}\", epoch)\n",
    "\n",
    "    # Enregistrer la précision dans TensorBoard\n",
    "    accuracy = 100. * correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    writer.add_scalar('Accuracy/test', accuracy, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97.54, 98.23, 97.91, 97.69, 98.5, 98.44, 98.5, 98.31, 98.37, 98.45]\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle\n",
    "torch.save(cnn_model.state_dict(), 'mnist_cnn_model.pth')\n",
    "\n",
    "print(accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
